# Seamlessly Deploy Cloud Native Apps - using Azure Arc and AKS



## Introduction

The purpose of this document is to guide users to *Deploy* various *Cloud Native Applications* seamlessly onto an <u>*AKS cluster*</u> without eedxplicitly containerizeing them!  This also implied that now developers/arhietcts coming from various domian and tech stack, do not necessarily need to know containers for deploying their applications; but still can get the benefits of Kubernetes based of eco-sytem, its scaling, its app

Now for deploying Cloud Native Applications on to K8s - we would use *Azure Arc for Kubernetes* as the service. This would help us to still work with our applications as-is on Cloud *(App Services, Functions, Logic Apps, Message Brokers like EventGrid* etc.) yet you can run it anywhere - be it on a *Managed Cluster like AKS* or an *Un-managed cluster like this one* Or *K8s clusters on any other Cloud* Or even *K8s bare metal clusters*.

### What the Document does

- Deep-dive on Azure Arc for K8s
- Create an Un-managed cluster using CAPZ templates
- Installs all Providers and Extensions needed by Azure Arc for K8s
- Creates and Deploys a sample *Web App* in NodeJs onto the CAPZ cluster
- Creates and Deploys a sample *Function App* in .NetCore onto the CAPZ cluster
- Creates and Deploys a sample *Logic App* workflow onto the CAPZ cluster
- Create an *EventGrid Topic* on Azure and Subscribes for any of the above services e.g. *Function App Endpoint*.
  - Push an event to EventGrid topic from the NodeJS WebApp
  - Check that the corresponding Subscription Endpoint (e.g. *Function App*) is fired!

### What the Document does NOT

- Deep-dive on [K8s](https://kubernetes.io/docs/home/) or [CAPZ](https://capz.sigs.k8s.io/)
- Deep-dive on [Azure Arc](https://docs.microsoft.com/en-us/azure/azure-arc/overview)
- Go thru the details of CAPZ cluster creation
- Explanation of the templates of CAPZ

![agent-connect](./Assets/agent-connect.png)



[*Ref: Azure Arc Components*](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-agent-architecture#deploy-agents-to-your-cluster)

- K8s Cluster can be anywhere - On-Prem, Azure or any other cloud
- *Azure Arc enabled K8s Service* sits on Azure and can connect all these clusters as a single control plane
- *Azure Arc enabled agent* makes outbound calls to push metadata information to Azure through *Azure Arc enabled K8s Service*
- *Azure Arc enabled agent* pulls information on Agent upgrade information and Configuration from Azure through *Azure Arc enabled K8s Service*

### Pre-requisites, Assumptions

- Knowledge on Containers, Serverless Functions, Logic App - *L200+*
- Knowledge on K8s  - *L200+*
- Knowledge on VSCode; Deploying applications through VSCode - L200+
- Some knowledge on Azure tools & services viz. *Azure CLI, KeyVault, VNET* etc. would help

### Repository Structure

#### Deployments

- ##### Helms

  - Contains helm charts for any deployment that user may want to do on the K8s cluster
  - For this exercise, for CAPZ cluster, only persistent volume is needed - **pv-chart**
    - **values-eg.yaml** - This is to deploy PersistentVolume for EventGrid - this would be explained later
    - **values-fs.yaml** - This is to deploy PersistentVolume for App Services - this would be explained later
  
- ##### Setup

  - All files that would be used during creation of CAPZ cluster; including the *Cluster Configuration* file
  - **KubeConfig** file to be generated by CAPZ runtime and to be used throughout this exercise to connect to the cluster

### Plan

- #### Create Jump Server resources

  - **master-workshop-rg**

    - **Virtual Network and Subnets** - Hosting *Jump Server*

    - **Jump Server VM**

      - Run through all commands to Create and Connect to the cluster

      - Run through all commands to configuere Cluster for Azure Arc

      - Run through all commands to deploy applications onto the Cluster

        

- #### Seggregate the workload into 3 Resource Groups as per the usage

  - **capz-workshop-rg**

    - Contains all Infrastructure components and services of the CAPZ cluster
      - **Master Node VM** - Control Pane of K8s
      - **Worker Node VMs** - 3 VMs to host app workloads
      - **Virtual Network and Subnets** - Hosting K8s cluster Nodes - *Master, Worker* and *Jump Server*
      - **Load Balancers** - Load Balancers for all the above VMs - Master and Worker
      - **NSGs** - for both Master Subnet and Worker Subnet
      - **Disks**
        - OS Disks - Master and Worker Nodes
        - ETCD Disks - Master Node

  - **arc-k8s-rg**

    - Contains Azure Arc components for K8s

      - ##### **Arc Connected Cluster** for the CAPZ cluster

        - Establishes a connectivity to API Server running on K8s Control Plane (*Master Node*) and ensures Arc is managing the k8s cluster on Azure

          

      - ##### **CustomLocation**
      
        - Acts as the Target location for deploying application and Data service instances on the k8s cluster. 
      
        - Each cluster would need one or more Custom Locations
      
        - Each **CustomLocation** can host multiple app and data instances
      
        - To be explained later in details
      
          
      
      - ##### App Service Kubernetes environment
      
        - Enables configuration common to apps in the custom location
        
        - This is required before any apps are deployed onto the cluster
        
          
  
  - **arc-services-rg**
  
    - Contains all microservices to be deployed onto K8s Cluster
      - **PostAPIApp**
      - **ReceiveMessageApp** Function App
      - **Workflow** Logic App
      - **capz-k8s-cluster-egt** Event Grid Topic
      
      

### Action

#### Management Resources

- **Create** Management Resource Group

  ```bash
  az group create -l eastus -n master-workshop-rg
  ```

-  **Create** VNet and SubNet for Jump Server VM

- **Create** a Jump Server VM - preferred is Ubuntu VM. The one used for this workshop was - 

   - OS - **Ubuntu-2004-LTS**
   - Size  - **Standard DS2 v2 (2 vcpus, 7 GiB memory)**
   
   *Also, this exercise has been tested with MacOS as well*

- **SSH** to the *Jump Server VM*

- **Install** following tools for creation and management of the cluster and its associated resources

   1. **Azure CLI**

   ```bash
   # Follow this link and install Azure CLI latest
   curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
   ```

   2. **Kubectl**

   ```bash
   curl -LO https://dl.k8s.io/release/v1.21.0/bin/linux/amd64/kubectl
   sudo install -o root -g root -m 0755 k /usr/local/bin/kubectl 
   ```

   3. **Kind** - *As Management Cluster for CAPZ*

      ```bash
      curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64
      chmod +x ./kind
      sudo mv ./kind /usr/local/bin
      ```

   4. **Helm**

      ```bash
      sudo snap install helm --classic
      
      OR,
      
      curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -
      sudo apt-get install apt-transport-https --yes
      echo "deb https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
      sudo apt-get update
      sudo apt-get install helm
      ```

   5. **ClusterCtl**

      ```bash
      curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.3.20/clusterctl-linux-amd64 -o clusterctl
      ```

   6. (Optional) **PowerShell Core**

      ```bash
      # Update the list of packages
      sudo apt-get update
      
      # Install pre-requisite packages.
      sudo apt-get install -y wget apt-transport-https software-properties-common
      
      # Download the Microsoft repository GPG keys
      wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb
      
      # Register the Microsoft repository GPG keys
      sudo dpkg -i packages-microsoft-prod.deb
      
      # Update the list of packages after we added packages.microsoft.com
      sudo apt-get update
      
      # Install PowerShell
      sudo apt-get install -y powershell
      
      # Inatall Az Modules
      Install-Module -Name Az -AllowClobber
      
      # Start PowerShell
      pwsh
      ```

- The **Jump Server** is now ready to be used for subsequent deployments   

#### Working Resources

![conceptual-cluster-connect](./Assets/conceptual-cluster-connect.png)

[Ref: Cluster Connect](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-cluster-connect#architecture)

- *cluster-connect* is a reverse-proxy agent running on the CAPZ cluster, established a session with *Azure Arc Service*
- Any call to K8s API server would be forwarded to *cluster-connect* agent which passes the request to *kube-aad-proxy*
- *kube-aad-proxy* reforms Azure AD authentication and if successful forwards the request to API Server for the intended cluster

1. **Set CLI** variables for easy usage and reference

   ```bash
   tenantId="<tenant_Id>"
   subscriptionId="<subscription_Id>"
   capzResourceGroup="capz-k8s-rg"
   arcK8sResourceGroup="arc-capz-k8s-rg"
   arcSvcResourceGroup="arc-capz-services-rg"
   clusterName="capz-k8s-cluster"
   location="eastus"
   baseFolderPath="<root_folder_path>/Deployments"
   ```

2. **Login** to Azure

   ```bash
   az login --tenant $tenantId
   ```

3. **Create** *Resource Groups* as described in the **Plan** section

   ```bash
   az group create -l eastus -n $capzResourceGroup
   az group create -l eastus -n $arcK8sResourceGroup
   az group create -l eastus -n $arcSvcResourceGroup
   
   ```

4. **Deploy** Management Cluster for CAPZ

   - **Kind** would be used for this exercise as it is very easy to set up!

   - Any K8s cluster can be used management cluster for CAPZ

     ```bash
     # Create the management cluster locally
     kind create cluster
     
     # Once created, check the cluster info of the Kind cluster
     kubectl cluster-info --context kind-kind
     ```

     

5. **Create** Service Principal for CAPZ cluster

   - **k8s-capz-sp** - Name of the service principal

     ```bash
     # Create service principal - k8s-capz-sp
     az ad sp create-for-rbac --skip-assignment --name http://k8s-capz-sp
     
     # Service Principal details
     {
       "appId": "<appId>",
       "displayName": "k8s-capz-sp",
       "name": "http://k8s-capz-sp",
       "password": "<password>",
       "tenant": "<tenantId>"
     }
     ```

6. Set **ENV** variables used by CAPZ installation

   ```bash
   export AZURE_SUBSCRIPTION_ID="<subscriptionId>"
   export AZURE_TENANT_ID="<tenantId>"
   export AZURE_CLIENT_ID="<appId>"
   export AZURE_CLIENT_SECRET="<apssword>"
   export AZURE_ENVIRONMENT="AzurePublicCloud"
   export AZURE_CONTROL_PLANE_MACHINE_TYPE="Standard_DS2_v2"
   export AZURE_NODE_MACHINE_TYPE="Standard_D8s_v3"
   export AZURE_LOCATION="eastus"
   
   export AZURE_SUBSCRIPTION_ID_B64="$(echo -n "$AZURE_SUBSCRIPTION_ID" | base64 | tr -d '\n')"
   export AZURE_TENANT_ID_B64="$(echo -n "$AZURE_TENANT_ID" | base64 | tr -d '\n')"
   export AZURE_CLIENT_ID_B64="$(echo -n "$AZURE_CLIENT_ID" | base64 | tr -d '\n')"
   export AZURE_CLIENT_SECRET_B64="$(echo -n "$AZURE_CLIENT_SECRET" | base64 | tr -d '\n')"
   ```

7. **Assign** *Role* for Service Principal

   - **Contributor** access to the Subscription

     ```bash
     # Create Role assignment - Contrubutor
     az role assignment create --role=Contributor --assignee=$AZURE_CLIENT_ID --scope=/subscriptions/$AZURE_SUBSCRIPTION_ID
     ```

8. **Initialize** the Azure Provider

   ```bash
   clusterctl init --infrastructure azure
   ```

9. **Create** Configuration Template for *Workload Cluster*

   - Modify values as appropriate 

   - This exercise would create a cluster with 1 *Master* Node and 3 *Worker* Nodes

     ```bash
     # Modify values as appropriate 
     clusterctl config cluster $clusterName --kubernetes-version v1.18.19 --control-plane-machine-count=1 --worker-machine-count=3 > capz-k8s-cluster.yaml
     
     # Apply cluster configuration file
     kubectl apply -f capz-k8s-cluster.yaml
     ```

10. **Deploy** *Workload Cluster*

    - **Check** status of Cluster creation

      ```bash
      kubectl get cluster --all-namespaces
      clusterctl describe cluster capz-k8s-cluster
      kubectl get kubeadmcontrolplane --all-namespaces
      kubectl get Machine -A
      ```

    - The CAPZ cluster creation would create following resources in the **$capzResourceGroup**

      - **Create** VNET - ***capz-k8s-cluster-vnet***
      - **Create** 2 SubNets
        - *Master* - ***capz-master-subnet***
        - *Worker* - ***capz-worker-subnet***
      - **Create** 1 *Master* VM, 3 *Worker* VMs
      - **Create** appropriate NSGs
      - **Create** Disks
        - **OS Disks** - Master and Worker Nodes
        - **ETCD Disks** - Master Node

11. Get **kubeconfig** for newly created CAPZ cluster

    ```bash
    clusterctl get kubeconfig $clusterName > capz-k8s-cluster.kubeconfig
    
    # Can be merged to the existing kubeconfig file Or operate from another location
    # If operating from another location use the alias as below
    alias k-capz="k --kubeconfig=$baseFolderPath/Setup/capz-k8s-cluster.kubeconfig"
    alias helm-capz="helm --kubeconfig=$baseFolderPath/Setup/capz-k8s-cluster.kubeconfig"
    ```

12. **Deploy** **Calico** Network plugin

    ```bash
    # Network Plugin - Calico
    k-capz apply -f https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-azure/master/templates/addons/calico.yaml
    
    # Get K8s cluster contexts
    k-capz config get-contexts
    
    # Get K8s cluster nodes, pods to check the status of the newly created cluster
    k-capz get no, po
    ```

13. **Set** *Azure Arc Extension* variables

    ```bash
    connectedClusterName="arc-capz-k8s"
    customLocationName="$clusterName-custom-location"
    appsvcExtensionName="$clusterName-ext-appsvc"
    appsvcExtensionNamespace="$clusterName-appsvc-ns"
    appsvcKubeEnvironmentName="$clusterName-appsvc-kube"
    ```

14. Add **connectedk8s** extension to Azure CLI

    ```bash
    az extension add --upgrade --yes --name connectedk8s
    az extension add --upgrade --yes --name k8s-extension
    az extension add --upgrade --yes --name customlocation
    
    az extension remove --name appservice-kube
    az extension add --yes --source "https://aka.ms/appsvc/appservice_kube-latest-py2.py3-none-any.whl"
    az extension show  -n appservice-kube -o table
    ```

15. **Register** *Providers* as required by *Azure Arc for K8s*

    ```bash
    # Register required Providers
    az provider register --namespace Microsoft.Kubernetes
    az provider register --namespace Microsoft.KubernetesConfiguration
    az provider register --namespace Microsoft.ExtendedLocation
    az provider register --namespace Microsoft.Web
    
    # Check Registration status of required Providers
    az provider show -n Microsoft.Kubernetes -o table
    az provider show -n Microsoft.KubernetesConfiguration -o table
    az provider show -n Microsoft.ExtendedLocation -o table
    az provider show -n Microsoft.Web -o table
    ```

16. **Connect** CAPZ cluster with Azure Arc

    ```bash
    # Get K8s cluster contexts
    k-capz config get-contexts
    
    # Connect K8s cluster with Azure Arc
    az connectedk8s connect -g $arcResourceGroupName -n $connectedClusterName \
    --kube-config $baseFolderPath/Setup/capz-k8s-cluster.kubeconfig \
    --kube-context capz-k8s-cluster-admin@capz-k8s-cluster
    ```

17. **Check** successful connectivity with Azure Arc

    ```bash
    # List Connected clusters
    az connectedk8s list --resource-group $arcResourceGroupName --output table
    
    # Check Deployments, Pods for the Arc connected cluster
    k-capz get deployments,pods -n azure-arc
    ```

18. **Create** a Public IP

    - Used by App Service Extension to create ELB service on CAPZ cluster

    - The Public Service is used by Application Services on Azure to communicate with the corrsponding Pods in the cluster

      ```bash
      # Create Public IP
      az network public-ip create -g $capzResourceGroup -n $clusterName-ext-appsvc-pip --sku STANDARD
      
      # Static Public IP
      staticIp=$(az network public-ip show -g $capzResourceGroup -n $clusterName-ext-appsvc-pip --output tsv --query ipAddress)
      ```
      
    - This is a [Limitation](https://docs.microsoft.com/en-us/azure/app-service/overview-arc-integration#public-preview-limitations) of the Public Preview feature to allow LoadBalancer service creation on the cluster and provide a public static IP. So, please make sure that the cluster does not have policy to prevent LoadBalancer service type to be deployed on the cluster

19. **Deploy** a *PersistentVolume* using Helm chart

    - Create a Storage Account on Azure. This is to be used as Persistent Storage for the K8s Cluster. Add one Azure FileShare with a size of 150Gi+

    - For this exercise, the PV is mapped to an *Azure FIle* storage account. Based on the K8s creation process, one can choose to handle this step in a different way - e.g. using Azure Disk as the mapped storage Or create and appropriate Storage class.

      *<u>This step is simple in a managed cluster scenario where in-built Storage classes are available as part of the cluster creation process</u>*

    - Used by *Application Services* Extension Pods, to be created later

    - The Pods would have a PVC with a request of 100Gi memory requirement

    - This PV would be Bound with the PVC and allow the Pod(s) to be created successfully

      ```bash
      k-capz create secret generic capz-k8s-fs-secret -n $appsvcExtensionNamespace --from-literal=azurestorageaccountname=<storage_account_name> --from-literal=azurestorageaccountkey=<storage_account_key>
      
      helm-capz install pv-arc-chart -n default $baseFolderPath/Helms/pv-chart/ -f $baseFolderPath/Helms/pv-chart/values-fs.yaml
      
      helm-capz upgrade pv-arc-chart -n default $baseFolderPath/Helms/pv-chart/ -f $baseFolderPath/Helms/pv-chart/values-fs.yaml
      
      # if you want to Uninstall the chart
      #helm-capz uninstall pv-arc-chart -n default
      ```

20. **Deploy** *App Service* Extension on the CAPZ cluster

    ```bash
    az k8s-extension create \
    --resource-group $arcResourceGroupName \
    --name $appsvcExtensionName \
    --cluster-type connectedClusters \
    --cluster-name $connectedClusterName \
    --extension-type 'Microsoft.Web.Appservice' \
    --release-train stable \
    --auto-upgrade-minor-version true \
    --scope cluster \
    --release-namespace $appsvcExtensionNamespace \
    --configuration-settings "Microsoft.CustomLocation.ServiceAccount=default" \
    --configuration-settings "appsNamespace=${appsvcExtensionNamespace}" \
    --configuration-settings "clusterName=${appsvcKubeEnvironmentName}" \
    --configuration-settings "loadBalancerIp=${staticIp}" \
    --configuration-settings "keda.enabled=true" \
    --configuration-settings "buildService.storageClassName=default" \
    --configuration-settings "buildService.storageAccessMode=ReadWriteOnce" \
    --configuration-settings "customConfigMap=${appsvcExtensionNamespace}/kube-environment-config" \
    --configuration-settings "envoy.annotations.service.beta.kubernetes.io/azure-load-balancer-resource-group=${capzResourceGroup}"
    ```

21. **Check** the status of the *App Service* Extension creation

    ```bash
    az k8s-extension show -c $connectedClusterName --cluster-type connectedClusters   -n $appsvcExtensionName -g $arcResourceGroupName
    ```

    ![conceptual-extensions](./Assets/conceptual-extensions.png)

    [*Ref: K8s-Extensions*](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-extensions#architecture)

    - Different extensions for different Service types to be deployed on the K8s Cluster
      - *App Service Extension* - Includes Web Apps/API Apps, Logic Apps, Function Apps
      - *EventGrid Extension* - Supports EventGrid Topic deployment
      - *Data Service Extension* - Supports deployment of SQL MI
    - *config-agent* - Keeps track fo new or updated extension resources. Multiple extensions can be added on the same cluster with different versions
    - *extension-manager* - Pulls Helm chart from Container Registry and deploys onto the K8s cluster

    

22. **Retrieve** the *ExtensionId* to be used in subsequent steps

    ```bash
    extensionId=$(az k8s-extension show \
      --cluster-type connectedClusters \
      --cluster-name $connectedClusterName \
      --resource-group $arcResourceGroupName \
      --name $appsvcExtensionName \
      --query id \
      --output tsv)
    echo $extensionId
    
    # Check Pods created in teh extensionNamespace
    k-capz get po -n $appsvcExtensionNamespace
    ```

23. **Create** *CustomLocation* with Azure Arc Connected Cluster

    - Every *Application Services* or *Data Services* would be deployed in the *CustomLocation* rather than an Azure Region/Location

    - This would ensure various types of application and data services can run together in same Arc Enabled Cluster

      ```bash
      connectedClusterId=$(az connectedk8s show --resource-group $arcResourceGroupName --name $connectedClusterName --query id --output tsv)
      echo $connectedClusterId
      
      az customlocation create \
      --resource-group $arcResourceGroupName \
      --name $customLocationName \
      --host-resource-id $connectedClusterId \
      --namespace $appsvcExtensionNamespace \
      --cluster-extension-ids $extensionId
      ```

      ![conceptual-arc-platform-layers](./Assets/conceptual-arc-platform-layers.png)

      [*Ref: Custom Location*](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-custom-locations#architecture)

      - Target Location for all services
      - As evident from the diagram - it is a layer around Azure Arc enabled K8s
      - Ideally one custom location per k8s cluster
      - Every allowed services on Azure Arc would be deployed to this Custom Location; instead of an Azure region 
      - Creates K8s *ClusterRoleBindings* and *RoleBindings*
      - This will ensure that Azure Arc enabled K8s can deploy necessary resources viz. pods, services, replicates, statefulsets etc.) for the deployed application services i.e. API Apps, FunctionApps, Logic Apps etc.

24. **Check** the status of CustomLocation creation process

    ```bash
    az customlocation show --resource-group $arcResourceGroupName --name $customLocationName
    ```

25. **Retrieve** the *CustomLocationId* to be used in subsequent steps

    ```bash
    customLocationId=$(az customlocation show \
    --resource-group $arcResourceGroupName \
    --name $customLocationName \
    --query id \
    --output tsv)
    echo $customLocationId 
    ```

26. **Create** *App Service Kube Environment* for the above *CustomLocation*

    - This is a collection of all *App Service Plans* and *App Services*

    - Please note that this is only needed for Application Services; for Data Services thsi would be performed by Data Controllers for Arc

      ```bash
      az appservice kube create \
      --resource-group $arcResourceGroupName \
      --name $appsvcKubeEnvironmentName \
      --custom-location $customLocationId \
      --static-ip $staticIp
      ```

27. **Check** *Kube Environment* creation process

    ```bash
    az appservice kube show \
    --resource-group $arcResourceGroupName \
    --name $appsvcKubeEnvironmentName
    ```



#### High level Architcture

Let us now delve into creating **Application Services** onto Azure and then deploying onto CAPZ cluster

![capz-arc-hight-level](./Assets/capz-arc-hight-level.png)

This is what we would like achieve in this exercise on the Custom Location created above:

- Deploy a NodeJS API App - **PostAPIApp**

- Deploy a .NetCore Function App - **ReceiveMessageApp**

- Deploy a Locally developed Blob triggered Logic App - **Workflow** Logic App

- Deploy an EventGrid topic **capz-k8s-cluster-egt**

- **Test**

  - Push an event to EventGrid topic from the NodeJS **PostAPIApp**
  - Check that the corresponding Subscription Endpoint (e.g. ***ReceiveMessageApp***) is fired!
  - Test **Workflow** Logic App with a Blob trigger

  

#### Use Case Flow

![capz-arc-usecase-1](./Assets/capz-arc-usecase-1.png)



#### App Services

![app-service1](./Assets/app-service1.png)

​										**<u>Deploy Web App from project workspace - VSCode</u>**

![app-service-2](./Assets/app-service-2.png)

​										**<u>Deploy Web App from Azure plugin - VSCode</u>**

- This execise uses a simple API App in NodeJS - **PostAPIApp** for this purpose. One can use any App Service or Web API for this purpose

- Visual Studio Code or Visual Studio both have easy integration with Azure Resource management. Any other IDE with appropriate plugins can be used as well. This exercise would use VSCode as an option

- Open App Service root folder in Visual Studio Code

- Right Click and **Deploy** to API App. Please note one can create the Web App/API App in the portal and then manage deployment from VSCode

- VSCode would ask for a new App to be Created Or Deploy on an existing one

- The Target Location step is extremely important - ***should be the <u>CustomLocation</u> created in earlier steps***

- Once the steps are completed, comeback to Azure CLI

- Check *Deployments* and/or *Pods* of the App Service Namespace in the K8s cluster. All Pods should be in the running state

- Go to Azure Portal and Check the App Service resource; in the Overview blade it will show up the Web API access URL

- Check the URL in te browser; use Postman or any REST client to call to test different paths of the API App

  

#### Function App



![function-2](./Assets/function-2.png)

​										**<u>Deploy Function App from project workspace - VSCode</u>**



![function-1](./Assets/function-1.png)

​										**<u>Deploy Function App from Azure plugin - VSCode</u>**

- This execise uses a simple *Http Triggerred* Azure Function in .NetCore - **PostMessageApp** for this purpose. One can use any type of Azure Function of their choice
- Visual Studio Code or Visual Studio both have easy integration with Azure Resource management. Any other IDE with appropriate plugins can be used as well. This exercise would use VSCode as an option
- Open Function App  root folder in Visual Studio Code
- Right Click and **Deploy** to Function App. Please note one can create the Function App in the portal and then manage deployment from VSCode
- VSCode would ask for a new App to be Created Or Deploy on an existing one
- The Target Location step is extremely important - ***should be the <u>CustomLocation</u> created in earlier steps***
- Once the steps are completed, comeback to Azure CLI
- Check *Deployments* and/or *Pods* of the App Service Namespace in the K8s cluster. All Pods should be in the running state
- Go to Azure Portal and Check the Function App  resource; in the Overview blade it will show up the Web API access URL
- Check the URL in te browser; use Postman or any REST client to call to test different paths of the Function App



#### Logic App

![logic-app-1](./Assets/logic-app-1.png)

​										**<u>Deploy Logic App from project workspace - VSCode</u>**



![logic-app-2](./Assets/logic-app-2.png)

​										**<u>Deploy Logic App from Azure plugin - VSCode</u>**

- This execise uses a simple *Blob Triggerred* Logic App Created Locally - **WorkflowApp** for this purpose
- Few points to note here on the choice of Creation path to Azure and subsequent Deployment onto K8s cluster
  - This Logic App type would be **<u>Standard</u>** and **Stateful** which is actually a **<u>Single Tenant Logic App</u>**; rather than the *Consumption* type Logic App which is *Multi-Tenant* Logic App
  - Currently the best way to achieve a seamless experiene end-to-end is to Create and Deploy Logic App Standard, Stateful type from Visual Studio Code itself
  - Not all triggers are available for **Standard** mode as of now
  - This exercise uses a simple *Blob trigger* for demonstration
- Visual Studio Code or Visual Studio both have easy integration with Azure Resource management. Any other IDE with appropriate plugins can be used as well. This exercise would use VSCode as an option
- Open Logic  App  root folder in Visual Studio Code
- Right Click and **Deploy** to Logic App
- VSCode would ask for a new App to be Created Or Deploy on an existing one
- The Target Location step is extremely important - ***should be the <u>CustomLocation</u> created in earlier steps***
- Once the steps are completed, comeback to Azure CLI
- Check *Deployments* and/or *Pods* of the App Service Namespace in the K8s cluster. All Pods should be in the running state
- Uplaod some blob images and check that the Logic App worlflow gets triggered

![logic-app-3](./Assets/logic-app-3.png)



![svc-pods-1](./Assets/svc-pods-1.png)

**<u>Pods in *App Service Extension namespace*</u>**

![svc-pods-2](./Assets/svc-pods-2.png)

**<u>Pods running the Application Services - *App Service, Azure Function and Logic App* (*Standard*)</u>**



#### Event Grid

![eventgrid-1](./Assets/eventgrid-1.png)

![eventgrid-2](./Assets/eventgrid-2.png)

![eventgrid-3](./Assets/eventgrid-3.png)

- This exercise uses a simple *Event Grid Topic* - **PostTopic** for this purpose

- Set local varibales in Azure CLI

  ```bash
  evgExtensionName="$clusterName-ext-eg"
  evgExtensionNamespace="$clusterName-evg-ns"
  evgTopicName="$clusterName-egt"
  evgSubscriptionName="$clusterName-evg-sub"
  ```

- **Deploy** a *PersistentVolume* using Helm chart

  - The Storage Account created in earlier step while preparing for App Services can be used here as well as Persistent Storage for the K8s Cluster

  - Used by *Application Services* Extension Pods, to be created later
  - The Pods would have a PVC with a request of 100Gi memory requirement
  - This PV would be Bound with the PVC and allow the Pod(s) to be created successfully

  ```bash
  helm-capz install pv-arc-eg-chart -n default $baseFolderPath/Helms/pv-chart/ -f $baseFolderPath/Helms/pv-chart/values-eg.yaml
  helm-capz upgrade pv-arc-eg-chart -n default $baseFolderPath/Helms/pv-chart/ -f $baseFolderPath/Helms/pv-chart/values-eg.yaml
  
  # if you want to Uninstall the chart
  #helm-capz uninstall pv-arc-eg-chart -n defaul
  ```

- **Create** *Event Grid* extension for Azure Arc on K8s

  This step is easy to be done through portal as there are many config options - https://docs.microsoft.com/en-us/azure/event-grid/kubernetes/install-k8s-extension

- Check the event Grid extension creation process

  ```bash
  az k8s-extension show -c $connectedClusterName --cluster-type connectedClusters \
  -n $evgExtensionName -g $arcK8sResourceGroup
  ```

  ![eventgrid-4](./Assets/eventgrid-4.png)

  **<u>Pods running *EventGrid namespace*</u>**

  

  ![eventgrid-5](./Assets/eventgrid-5.png)

  **<u>Arc Enabled Services</u>**

- Make sure to Select the EventGrid service from the dropdown before creating the EventGrid Topic - otherwise Topic creation would fail with Forbidden error; as it would not recognize the target deployment Location - which should be the *Custom Location* in the case

- Visual Studio Code or Visual Studio both DONOT have integration with Azure Arc flavour for EventGrid as of now. So, creating the Topic in portal is the only option as of now 

- The Target Location step is extremely important - ***should be the <u>CustomLocation</u> created in earlier steps***

- **Check** *Topic* details

  ```bash
  topicId=$(az eventgrid topic show --name $evgTopicName --resource-group $arcSvcResourceGroup --query id -o tsv)
  echo $topicId
  ```

- Create *Event Subscription* for the above topic

  ```bash
  az eventgrid event-subscription create --name $evgSubscriptionName --source-resource-id $topicId \
  --endpoint <event_subscription_endpoint>
  
   # e.g. function app endpoint that we had created earlier
  ```

- Check *Deployments* and/or *Pods* of the EventGrid Namespace in the K8s cluster. All Pods should be in the *running* state

  ```bash
  k-capz get po -n $eventGridnamespace
  ```

- Get EventGrid **Endpoint** details

  ```bash
  eventGridEndpoint=$(az eventgrid topic show --name $evgTopicName -g $arcSvcResourceGroup --query "endpoint" --output tsv)
  ```

- Get EventGrid **Key** details

  ```bash
  eventGridKey=$(az eventgrid topic key list --name $evgTopicName -g $arcSvcResourceGroup --query "key1" --output tsv)
  ```

- Following piece of code would be executed when a POST call is made for the **/upload** endpoint

  - Send the EventGrid **Endpoint** in Http request header as key - **ep**; e.g. ***ep = "<value>"***
  - Send the EventGrid **Key** in Http request header as key - **key**; e.g. ***key = "<value>"***
  - Only supported Event Schema as of now - **CloudEvent Schema**; *EventGrid Schema* will not work currently!

  ```javascript
  express.post("/upload", async (request, response) =>
  {
  
      try
      {
  
          const requestHeaders = request.headers;
          const evgHeaders =
          {
  
              "Content-Type": "application/cloudevents-batch+json",
              "aeg-sas-key": requestHeaders.key
  
          };
  
          const config =
          {
  
              headers: evgHeaders
  
          };
  
          const data =
          { 
              "specversion": "1.0",
              "type" : "orderCreated",
              "source": "myCompanyName/us/webCommerceChannel/myOnlineCommerceSiteBrandName",
              "id" : "eventId-n",
              "time" : "2020-12-25T20:54:07+00:00",
              "subject" : "account/acct-123224/order/o-123456",
              "dataSchema" : "1.0",
              "data" : {
                 "orderId" : "123892",
                 "orderType" : "PO",
                 "reference" : "https://www.myCompanyName.com/orders/123"
                }
          };
  
          const result = await Axios.post(requestHeaders.ep, [data], config);
          console.log(result);
          response.send("PostAPIApp - " + result.data);
          
      }
      catch(exception)
      {
  
          response.send("Error - " + exception.message);
  
      }
  ```

  

- This in-turn calls the subscription endpoint; check if the **PostMessageApp** Function being called


#### Additional Points

- The same exercise would work for AKS cluster also almost as-is
- [PersistentVolume](#Helms) Step through Helm chart would NOT be needed. AKS being a mangled Cluster - it would come-up with a set of *Storage Classes*
- Repository for this [Source Code](https://github.com/monojit18/K8s-Arc.git)

## References

**Azure Arc Enabled K8s**:

- https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/
- https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-agent-architecture
- https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-extensions
- https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-cluster-connect
- https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-custom-locations

**AppServices on K8s**

- https://docs.microsoft.com/en-us/azure/app-service/quickstart-arc
- https://docs.microsoft.com/en-us/azure/app-service/overview-arc-integration
- https://docs.microsoft.com/en-us/azure/app-service/manage-create-arc-environment

**EventGrid on K8s** 

- https://docs.microsoft.com/en-us/azure/event-grid/kubernetes/
- https://docs.microsoft.com/en-us/azure/event-grid/kubernetes/create-topic-subscription

**DataServices on K8s**

- https://docs.microsoft.com/en-us/azure/azure-arc/data/overview
- https://docs.microsoft.com/en-us/azure/azure-arc/data/create-data-controller-direct-cli
- https://docs.microsoft.com/en-us/azure/azure-arc/data/create-sql-managed-instance













